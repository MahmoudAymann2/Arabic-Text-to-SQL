# -*- coding: utf-8 -*-
"""Arabic-Text-to-SQL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MK0G8Dj1W94lIXhbN27JAc-9d8Fa88vS
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

mahmoudayman2_arabic_text_to_sql_path = kagglehub.dataset_download('mahmoudayman2/arabic-text-to-sql')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Install Dependencies"""

!pip install transformers torch sentencepiece evaluate

"""# Import Libraries"""

import json
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import (
    T5Tokenizer, T5ForConditionalGeneration,
    Trainer, TrainingArguments, DataCollatorForSeq2Seq
)
import evaluate
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import os
os.environ["WANDB_DISABLED"] = "true"

from google.colab import drive
drive.mount('/content/drive')

import json
import pandas as pd

DATASET_PATH = "/content/drive/MyDrive/P2_NLP/AR_spider.jsonl"

data = []

with open(DATASET_PATH, 'r', encoding='utf-8') as file:
    for line in file:
        try:
            record = json.loads(line)
            data.append(record)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON on line: {line}")
            print(e)

df = pd.DataFrame(data)

print(df.head())

print("\nMissing values in each column:\n", df.isnull().sum())

print("\nDataset information:\n")
df.info()

import os
import sqlite3
import pandas as pd

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Adjust this path to match the folder structure in your Google Drive
SCHEMA_PATH = "/content/drive/MyDrive/P2_NLP/database"
def extract_schema(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = cursor.fetchall()
    schema = {}
    for table in tables:
        table_name = table[0]
        cursor.execute(f"PRAGMA table_info({table_name});")
        columns = cursor.fetchall()
        schema[table_name] = [column[1] for column in columns]
    conn.close()
    return schema

# Make sure your DataFrame 'df' is already defined and includes 'db_id'
db_schemas = {}
for db_id in df['db_id'].unique():
    db_file = os.path.join(SCHEMA_PATH, db_id, f"{db_id}.sqlite")
    if os.path.exists(db_file):
        db_schemas[db_id] = extract_schema(db_file)
    else:
        print(f"Schema file for {db_id} not found at {db_file}.")

# Display schema for a sample db_id
if db_schemas:
    sample_db_id = list(db_schemas.keys())[0]
    print(f"Schema for {sample_db_id}:\n", db_schemas[sample_db_id])
else:
    print("No schemas loaded.")

from sklearn.model_selection import train_test_split
import pandas as pd

train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)

# Split the temporary dataset equally into validation 10% and test 10% datasets
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)

# Verify the sizes of each dataset
print(f"Training set size: {len(train_df)}")
print(f"Validation set size: {len(val_df)}")
print(f"Test set size: {len(test_df)}")

import json

def convert_to_chat_format(df, save_path):
    messages = []
    for _, row in df.iterrows():
        arabic_question = row['arabic'].strip()
        sql_query = row['query'].strip()

        # Ensure both question and query are present
        if arabic_question and sql_query:
            message = {
                "messages": [
                    {"role": "user", "content": arabic_question},
                    {"role": "assistant", "content": sql_query}
                ]
            }
            messages.append(message)

    # Save each message as one line in a JSONL file
    with open(save_path, 'w', encoding='utf-8') as f:
        for message in messages:
            f.write(json.dumps(message, ensure_ascii=False) + '\n')

# Convert and save in Colab environment
convert_to_chat_format(train_df, "/content/train.jsonl")
convert_to_chat_format(val_df, "/content/val.jsonl")

print("Chat-formatted train and val files saved to /content.")

import json

def load_jsonl(path):
    with open(path, 'r', encoding='utf-8') as f:
        return [json.loads(line) for line in f]

# Paths to your prepared files
train_path = "/content/train.jsonl"
val_path = "/content/val.jsonl"

# Load
train_data = load_jsonl(train_path)
val_data = load_jsonl(val_path)

# Show shapes
print(f"Train set size: {len(train_data)}")
print(f"Validation set size: {len(val_data)}")

# Show example
print("\n Sample train entry:")
print(json.dumps(train_data[10], ensure_ascii=False, indent=2))

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from huggingface_hub import login
login("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")

!pip install -q datasets peft accelerate trl

#! pip install -U bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
import torch

# Model Name
model_name = "mistralai/Mistral-7B-Instruct-v0.1"

# Tokenizer Setup
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Ensure padding is set

# Quantization Config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Enable 4-bit loading
    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation
    bnb_4bit_use_double_quant=True,  # Optionally, enable double quantization
)

# Load the Model with Quantization Config
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16,
)

from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model

# Prepare the model for k-bit training (after loading it with 4-bit or 8-bit config)
model = prepare_model_for_kbit_training(model)

# Define LoRA configuration
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Adjust depending on your model architecture
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_2_SEQ_LM"  # Use this for models like T5 or MT5
)

# Wrap the model with LoRA
model = get_peft_model(model, lora_config)

from datasets import load_dataset

data = load_dataset("json", data_files={
    "train": "/content/train.jsonl",
    "validation": "/content/val.jsonl"
})

from trl import SFTConfig, SFTTrainer

sft_config = SFTConfig(
    max_seq_length=1024,                          # Long enough for SQL queries
    output_dir="/content/mistral-finetuned-ar2sql",  # Colab-compatible path

    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,

    num_train_epochs=3,

    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,

    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    weight_decay=0.01,

    bf16=False,
    fp16=True,
    logging_dir="/content/logs",                  # Colab-compatible path
    logging_strategy="steps",
    logging_steps=50,

    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=data["train"],
    eval_dataset=data["validation"],
    peft_config=lora_config
)

trainer.train()

# Evaluate the model after training completes
eval_results = trainer.evaluate()

print("Evaluation Results:")
for k, v in eval_results.items():
    print(f"{k}: {v}")

import re

def clean_sql_output(raw_output):
    """
    Extracts only the SQL portion from the model's generated output.
    Removes Arabic prompt echo, special tokens, or formatting noise.
    """
    # Remove anything before "SELECT" (or other SQL keywords)
    match = re.search(r"(SELECT|WITH|INSERT|UPDATE|DELETE)\b.*", raw_output, re.IGNORECASE | re.DOTALL)
    if match:
        return match.group(0).strip()
    else:
        # Fallback: return raw if no SQL found
        return raw_output.strip()

def generate_sql(prompt, max_new_tokens=200):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return clean_sql_output(decoded)
# Ex
test_input = "كم عدد الموظفين في قسم المبيعات؟"
print("Arabic:", test_input)
print("SQL:", generate_sql(test_input))

#Test
test_input = "ما هو إجمالي مبلغ المنح للأبحاث؟"
print("Arabic:", test_input)
print("SQL:", generate_sql(test_input))

#Test
test_input = "كم عدد الطلاب المسجلين في قسم الحاسوب؟"
print("Arabic:", test_input)
print("SQL:", generate_sql(test_input))

#Test
test_input = "ما هي أسماء جميع الأغاني التي تمتلك تقييمًا أقل من بعض الأغاني في نوع البلوز؟"
print("Arabic:", test_input)
print("SQL:", generate_sql(test_input))

#Test
test_input = "ما هي أسماء الأساتذة الذين يدرّسون مادة الرياضيات؟"
print("Arabic:", test_input)
print("SQL:", generate_sql(test_input))

#Test
test_input = "ما هي المواد التي يدرسها الدكتور أحمد؟"
print("Arabic:", test_input)
print("SQL:", generate_sql(test_input))

# Define the directory where the model will be saved
output_dir = "/content/mistral-finetuned-ar2sql"

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"Model and tokenizer saved to {output_dir}")

